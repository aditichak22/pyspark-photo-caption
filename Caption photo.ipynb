{"cells":[{"cell_type":"code","source":["pip install tensorflow"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e699094-0639-480e-8597-4dd4b7a06834","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\nCollecting libclang&gt;=9.0.1\n  Using cached libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\nRequirement already satisfied: numpy&gt;=1.14.5 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.19.2)\nCollecting wrapt&gt;=1.11.0\n  Using cached wrapt-1.13.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (84 kB)\nCollecting gast&lt;0.5.0,&gt;=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting astunparse&gt;=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting opt-einsum&gt;=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: protobuf&gt;=3.9.2 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (3.17.2)\nCollecting google-pasta&gt;=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting absl-py&gt;=0.4.0\n  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.21.0\n  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\nCollecting flatbuffers&lt;3.0,&gt;=1.12\n  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\nCollecting termcolor&gt;=1.1.0\n  Using cached termcolor-1.1.0-py3-none-any.whl\nCollecting tensorflow-estimator&lt;2.8,~=2.7.0rc0\n  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\nCollecting tensorboard~=2.6\n  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\nRequirement already satisfied: six&gt;=1.12.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nCollecting keras-preprocessing&gt;=1.1.1\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nCollecting typing-extensions&gt;=3.6.6\n  Using cached typing_extensions-4.0.0-py3-none-any.whl (22 kB)\nCollecting grpcio&lt;2.0,&gt;=1.24.3\n  Using cached grpcio-1.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\nCollecting keras&lt;2.8,&gt;=2.7.0rc0\n  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\nCollecting h5py&gt;=2.9.0\n  Using cached h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\nCollecting werkzeug&gt;=0.11.15\n  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /databricks/python3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow) (2.25.1)\nCollecting google-auth&lt;3,&gt;=1.6.3\n  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6-&gt;tensorflow) (52.0.0)\nCollecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nCollecting tensorboard-plugin-wit&gt;=1.6.0\n  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\nCollecting markdown&gt;=2.6.8\n  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\nCollecting rsa&lt;5,&gt;=3.1.4\n  Using cached rsa-4.8-py3-none-any.whl (39 kB)\nCollecting cachetools&lt;5.0,&gt;=2.0.0\n  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\nCollecting pyasn1-modules&gt;=0.2.1\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting requests-oauthlib&gt;=0.7.0\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata&gt;=4.4\n  Using cached importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\nCollecting zipp&gt;=0.5\n  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\nCollecting pyasn1&lt;0.5.0,&gt;=0.4.6\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (2.10)\nCollecting oauthlib&gt;=3.0.0\n  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\nSuccessfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 typing-extensions-4.0.0 werkzeug-2.0.2 wrapt-1.13.3 zipp-3.6.0\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f345be14-dae7-44db-81c9-c38da0d79186/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\nCollecting libclang&gt;=9.0.1\n  Using cached libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\nRequirement already satisfied: numpy&gt;=1.14.5 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.19.2)\nCollecting wrapt&gt;=1.11.0\n  Using cached wrapt-1.13.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (84 kB)\nCollecting gast&lt;0.5.0,&gt;=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting astunparse&gt;=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting opt-einsum&gt;=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: protobuf&gt;=3.9.2 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (3.17.2)\nCollecting google-pasta&gt;=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting absl-py&gt;=0.4.0\n  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.21.0\n  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\nCollecting flatbuffers&lt;3.0,&gt;=1.12\n  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\nCollecting termcolor&gt;=1.1.0\n  Using cached termcolor-1.1.0-py3-none-any.whl\nCollecting tensorflow-estimator&lt;2.8,~=2.7.0rc0\n  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\nCollecting tensorboard~=2.6\n  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\nRequirement already satisfied: six&gt;=1.12.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nCollecting keras-preprocessing&gt;=1.1.1\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nCollecting typing-extensions&gt;=3.6.6\n  Using cached typing_extensions-4.0.0-py3-none-any.whl (22 kB)\nCollecting grpcio&lt;2.0,&gt;=1.24.3\n  Using cached grpcio-1.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\nCollecting keras&lt;2.8,&gt;=2.7.0rc0\n  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\nCollecting h5py&gt;=2.9.0\n  Using cached h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\nCollecting werkzeug&gt;=0.11.15\n  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /databricks/python3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow) (2.25.1)\nCollecting google-auth&lt;3,&gt;=1.6.3\n  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6-&gt;tensorflow) (52.0.0)\nCollecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nCollecting tensorboard-plugin-wit&gt;=1.6.0\n  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\nCollecting markdown&gt;=2.6.8\n  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\nCollecting rsa&lt;5,&gt;=3.1.4\n  Using cached rsa-4.8-py3-none-any.whl (39 kB)\nCollecting cachetools&lt;5.0,&gt;=2.0.0\n  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\nCollecting pyasn1-modules&gt;=0.2.1\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting requests-oauthlib&gt;=0.7.0\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata&gt;=4.4\n  Using cached importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\nCollecting zipp&gt;=0.5\n  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\nCollecting pyasn1&lt;0.5.0,&gt;=0.4.6\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow) (2.10)\nCollecting oauthlib&gt;=3.0.0\n  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\nSuccessfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 typing-extensions-4.0.0 werkzeug-2.0.2 wrapt-1.13.3 zipp-3.6.0\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f345be14-dae7-44db-81c9-c38da0d79186/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23950720-2ab8-4d44-8e23-50cb82847ef3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting nltk\n  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\nCollecting click\n  Using cached click-8.0.3-py3-none-any.whl (97 kB)\nCollecting regex&gt;=2021.8.3\n  Using cached regex-2021.11.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nInstalling collected packages: tqdm, regex, click, nltk\nSuccessfully installed click-8.0.3 nltk-3.6.5 regex-2021.11.10 tqdm-4.62.3\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f345be14-dae7-44db-81c9-c38da0d79186/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting nltk\n  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\nCollecting click\n  Using cached click-8.0.3-py3-none-any.whl (97 kB)\nCollecting regex&gt;=2021.8.3\n  Using cached regex-2021.11.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nInstalling collected packages: tqdm, regex, click, nltk\nSuccessfully installed click-8.0.3 nltk-3.6.5 regex-2021.11.10 tqdm-4.62.3\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f345be14-dae7-44db-81c9-c38da0d79186/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# From the Keras.io docs, code for using preprocessing layers APIs for image augmentation.\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\n\nimageAugmentation = Sequential(\n    [\n        layers.RandomRotation(factor=0.15),\n        layers.RandomTranslation(height_factor=0.1, width_factor=0.1), \n        layers.RandomFlip(),\n        layers.RandomContrast(factor=0.1),\n    ],\n    name=\"imageAugmentation\",\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25dead2f-c251-4e7c-9646-67060c6ed64f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nfrom pickle import dump\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom keras.layers import InputLayer, Activation\nfrom keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\nfrom tensorflow.keras.optimizers import Adam \n\n# Image size required for EfficientNet\nIMG_SIZE = 224\n\n# extract features from each image in the directory\ndef extractFeatures(directory):\n\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    x = imageAugmentation(inputs)\n\n# This option excludes the final Dense layer that turns 1280 features on the penultimate layer into prediction of the 1000 ImageNet classes. \n# Replacing the top layer with custom layers allows using EfficientNet as a feature extractor in a transfer learning workflow.\n\n    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n  \n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    print(model.summary())\n    features = dict()\n    trainImagesList = os.listdir(directory)\n    for name in trainImagesList:\n        filename = directory + name\n        print(filename)\n        image = load_img(filename, target_size=(224, 224))\n        image = img_to_array(image)\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        feature = model.predict(image, verbose=0)\n        image_id = name.split('.')[0]\n        features[image_id] = feature\n    return features\n\ndirectory = \"/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/flickr30k_images/flickr30k_images/\"\nfeatures = extractFeatures(directory)\nprint('Extracted Features: %d' % len(features))\ndump(features, open('/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl', 'wb'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a8b7c75-4092-4694-bfe5-83d9c07c4901","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1820641264163062&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     42</span> directory <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/flickr30k_images/flickr30k_images/&#34;</span>\n<span class=\"ansi-green-fg\">---&gt; 43</span><span class=\"ansi-red-fg\"> </span>features <span class=\"ansi-blue-fg\">=</span> extractFeatures<span class=\"ansi-blue-fg\">(</span>directory<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;Extracted Features: %d&#39;</span> <span class=\"ansi-blue-fg\">%</span> len<span class=\"ansi-blue-fg\">(</span>features<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     45</span> dump<span class=\"ansi-blue-fg\">(</span>features<span class=\"ansi-blue-fg\">,</span> open<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;wb&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-1820641264163062&gt;</span> in <span class=\"ansi-cyan-fg\">extractFeatures</span><span class=\"ansi-blue-fg\">(directory)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span> <span class=\"ansi-green-fg\">def</span> extractFeatures<span class=\"ansi-blue-fg\">(</span>directory<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span> \n<span class=\"ansi-green-fg\">---&gt; 18</span><span class=\"ansi-red-fg\">     </span>inputs <span class=\"ansi-blue-fg\">=</span> layers<span class=\"ansi-blue-fg\">.</span>Input<span class=\"ansi-blue-fg\">(</span>shape<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">(</span>IMG_SIZE<span class=\"ansi-blue-fg\">,</span> IMG_SIZE<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span>     x <span class=\"ansi-blue-fg\">=</span> imageAugmentation<span class=\"ansi-blue-fg\">(</span>inputs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     20</span> \n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;layers&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;layers&#39; is not defined","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1820641264163062&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     42</span> directory <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/flickr30k_images/flickr30k_images/&#34;</span>\n<span class=\"ansi-green-fg\">---&gt; 43</span><span class=\"ansi-red-fg\"> </span>features <span class=\"ansi-blue-fg\">=</span> extractFeatures<span class=\"ansi-blue-fg\">(</span>directory<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;Extracted Features: %d&#39;</span> <span class=\"ansi-blue-fg\">%</span> len<span class=\"ansi-blue-fg\">(</span>features<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     45</span> dump<span class=\"ansi-blue-fg\">(</span>features<span class=\"ansi-blue-fg\">,</span> open<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;wb&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-1820641264163062&gt;</span> in <span class=\"ansi-cyan-fg\">extractFeatures</span><span class=\"ansi-blue-fg\">(directory)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span> <span class=\"ansi-green-fg\">def</span> extractFeatures<span class=\"ansi-blue-fg\">(</span>directory<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span> \n<span class=\"ansi-green-fg\">---&gt; 18</span><span class=\"ansi-red-fg\">     </span>inputs <span class=\"ansi-blue-fg\">=</span> layers<span class=\"ansi-blue-fg\">.</span>Input<span class=\"ansi-blue-fg\">(</span>shape<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">(</span>IMG_SIZE<span class=\"ansi-blue-fg\">,</span> IMG_SIZE<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span>     x <span class=\"ansi-blue-fg\">=</span> imageAugmentation<span class=\"ansi-blue-fg\">(</span>inputs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     20</span> \n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;layers&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pickle import dump\nphotoFeats = pd.read_pickle('/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl')\n\n# Preserve key,value ordering\nkeys = list(photoFeats.keys())\nvalues = list(photoFeats.values())\nvalues = [x[0] for x in values] #each value is a list of 1 item\n\n# GlobalAveragePooling2D significantly decreases the size of each featurized image\nimgInput = tf.keras.layers.Input(shape=(7,7,1280)) # shape (7,7,1280) from EfficiencyNetB0\nimgAvgPool = tf.keras.layers.GlobalAveragePooling2D()(imgInput) # shape (1280) output\npoolModel = tf.keras.Model(inputs=[imgInput], outputs=[imgAvgPool]) \n# Apply the pooling\nvaluesTensor = tf.convert_to_tensor(values, dtype=tf.float32)\nnewValues = poolmodel.predict(valuesTensor)\n\n# Save the new features to disk\nfeaturesPooled =  dict(zip(keys, newValues))\ndump(featuresPooled, open('/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/featuresPooled.pkl', 'wb'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b57b734-c2ce-4bdb-bb17-224d48f55182","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Maps the list of images to the appropiate captions\ndef imagesMapCaption(train_images_list, train_captions):\n    mappings = {}\n    for i in train_images_list:\n      caps = []\n      captions = train_captions[train_captions['image_name'] == i]['comment']\n      for caption in captions:\n        caps.append(caption)\n      mappings[i] = caps\n    return mappings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48fc6a12-b94a-45d1-93a4-2708a8161e86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import string\nfrom string import digits\n\n# Does preprocessing and removes punctuation and numerics\ndef formatCaptions(captionMappings):\n  for image in captionMappings:\n    imageCaptions = captionMappings[image]\n#     print(imageCaptions)\n    fCaptions = []\n    for c in imageCaptions:\n      print(\"c\", c)\n      captionNoNumeric = c.translate(str.maketrans('', '', digits)).split(\" \")\n      captionFormat = \" \".join(list(filter(lambda x : len(x) > 1, captionNoNumeric)))\n#       print(captionNoNumeric)\n      caption = captionFormat.lower().strip()\n      captionNoPunct = caption.translate(str.maketrans('', '', string.punctuation))\n      captionSeq = \"<begofseq> \" + captionNoPunct + \" <endofseq>\"\n      fCaptions.append(captionSeq)\n    captionMappings[image] = fCaptions\n  return captionMappings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18bef8f9-5b9c-41d5-a41d-b3d0eff4e874","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import tensorflow as tf\nimport numpy as np\n\n#\n# LSTM\n#\n\n\n\n# Gate equations adapted from wikipedia definition of LSTM.\n# Intuitive purpose of gates adapted from colah's LSTM blog.\n\n#f forget    = sigma(W_f*x  +  U_f*ouput_prev  +  b_f)    :decides how much of cell_state to keep/forget\n#i input     = sigma(W_i*x  +  U_i*ouput_prev  +  b_i)    :decides which new candidate values to remember\n#s select    = sigma(W_s*x  +  U_s*ouput_prev  +  b_s)    :decides what portions of the cell_state to output\n#c candidate =  tanh(W_c*x  +  U_c*ouput_prev  +  b_c)    :creates new candidate values to be added to cell_state\n#cell_state = (forget*cell_prev)+(input*candidate) \n#output = select*tanh(cell_state)\n\n# Standard notation seems to use \"output\" instead of \"select\".\n# I decided to use \"select\" because this intermediary gate's function is not to BE the output,\n# but to instead select which portions of the cell_state to output.\n\n\n\n\n# Using the keras API to write the LSTM layer so that it works well with the rest of the network.\n# Particularly taking instruction from keras's MinimalRNNCell example class.\nclass LSTM_Cell(tf.keras.layers.Layer):\n    # For simplicity, the LSTM cell receives as input the 128 length text/linguistic feature vector.\n    # Any intermediate results of the LSTM are also 128 length, as well as the output. (Assuming called with size=128)\n\n    def __init__(self, size, **kwargs):\n        self.size = size\n        self.state_size = [self.size, self.size] # two 128-length vectors, not a 128x128 matrix\n        self.output_size = self.size\n        super(LSTM_Cell, self).__init__(**kwargs)\n        \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'size': self.size,\n        })\n        return config\n    \n    def predict(self, X):\n        return self.model.predict(X)\n    \n    def build(self, input_shape):\n        \n\n        # Weights (matrix) multiplying the input, x, where x=the text features\n        self.W_f = self.add_weight(shape=(input_shape[-1],self.output_size),name='W_f',initializer='glorot_uniform')\n        self.W_i = self.add_weight(shape=(input_shape[-1],self.output_size),name='W_i',initializer='glorot_uniform')\n        self.W_s = self.add_weight(shape=(input_shape[-1],self.output_size),name='W_s',initializer='glorot_uniform')\n        self.W_c = self.add_weight(shape=(input_shape[-1],self.output_size),name='W_c',initializer='glorot_uniform')\n        \n        # Weights (matrix) multiplying the recursive cell output, output_prev\n        self.U_f = self.add_weight(shape=(self.output_size,self.output_size),name='U_f',initializer='glorot_uniform')\n        self.U_i = self.add_weight(shape=(self.output_size,self.output_size),name='U_i',initializer='glorot_uniform')\n        self.U_s = self.add_weight(shape=(self.output_size,self.output_size),name='U_s',initializer='glorot_uniform')\n        self.U_c = self.add_weight(shape=(self.output_size,self.output_size),name='U_c',initializer='glorot_uniform')\n        \n        # Bias weights (vector)\n        # For b_f, the bias for the forget gate (where 0=forget), starts at 1 so that default is to not forget. \n        self.b_f = self.add_weight(shape=(self.output_size,),name='b_f',initializer='ones')\n        self.b_i = self.add_weight(shape=(self.output_size,),name='b_i',initializer='zeros')\n        self.b_s = self.add_weight(shape=(self.output_size,),name='b_s',initializer='zeros')\n        self.b_c = self.add_weight(shape=(self.output_size,),name='b_c',initializer='zeros')\n        \n        self.built = True\n\n    def call(self, inputs, states):\n        cell_prev = states[0]\n        output_prev = states[1]\n        \n        \n        #f forget    = sigma(W_f*x  +  U_f*ouput_prev  +  b_f)    :decides how much of cell_state to keep/forget\n        f = tf.keras.backend.dot(inputs, self.W_f) + tf.keras.backend.dot(output_prev, self.U_f) + self.b_f\n        f = tf.keras.backend.sigmoid(f)\n        \n        #i input     = sigma(W_i*x  +  U_i*ouput_prev  +  b_i)    :decides which new candidate values to remember\n        i = tf.keras.backend.dot(inputs, self.W_i) + tf.keras.backend.dot(output_prev, self.U_i) + self.b_i\n        i = tf.keras.backend.sigmoid(i)\n        \n        #s select    = sigma(W_s*x  +  U_s*ouput_prev  +  b_s)    :decides what portions of the cell_state to output\n        s = tf.keras.backend.dot(inputs, self.W_s) + tf.keras.backend.dot(output_prev, self.U_s) + self.b_s\n        s = tf.keras.backend.sigmoid(s)\n        \n        #c candidate =  tanh(W_c*x  +  U_c*ouput_prev  +  b_c)    :creates new candidate values to be added to cell_state\n        c = tf.keras.backend.dot(inputs, self.W_c) + tf.keras.backend.dot(output_prev, self.U_c) + self.b_c\n        c = tf.keras.backend.tanh(c)\n        \n        #cell_state = (forget*cell_prev)+(input*candidate)\n        #element-wise multiplication and addition \n        cell_state = tf.add( tf.multiply(f, cell_prev), tf.multiply(i, c) )\n        \n        #output = select*tanh(cell_state)\n        output = tf.multiply(s, tf.keras.backend.tanh(cell_state))\n\n        state = (cell_state, output)\n        return output, state\n    \n    \n\n#\n# Merge architecture Neural Network.\n#\n\n\n# Define the layers of the neural network. Assuming EfficiencyNetB0 (ENB0) as the image CNN\ndef mergeModel(vocab_size, max_num_words):\n  # vocab_size is the number of unique words in the set of training captions.\n  # max_num_words is the length of the longest wordcount caption in the training set.\n\n  # The merge model can also be described as an encoder-decoder model\n\n  #\n  # Encoders\n  #\n\n  # Encode image features from CNN ENB0's output to shape (128).\n  # The https://arxiv.org/abs/1708.02043 paper suggests 128 is optimal for flikr30k out of 128, 256, 512.\n  # Regadless of optimal accuracy, 128 will have fewer parameters and be faster to train, so we prefer it.\n  img_input_pooled_features = tf.keras.layers.Input(shape=(1280))\n  # Dropout layers discard some of the weights to help prevent overfitting the training data.\n  img_avg_pool_regularized = tf.keras.layers.Dropout(0.25)(img_input_pooled_features) # shape (1280)\n  img_128_features = tf.keras.layers.Dense(128, activation='relu')(img_avg_pool_regularized) # shape (128)\n\n  # Encode linguistic/text sequence features.\n  # Reccurrent Neural Network (RNN) works well for sequences.\n  # LSTM as the RNN to give improved long term memory, i.e. try to not forget how the sentence started.\n  txt_input = tf.keras.layers.Input(shape=(max_num_words,)) # shape (max_num_words)\n  # An embedding layer converts an input word to a one-hot vector of length vocab_size and is then\n  # densely (fully) connected to a desired output feature vector length (128).\n  # This is similar in function, after training, to a word vectorizor like word2vec.\n  txt_embeding = tf.keras.layers.Embedding(vocab_size, 128, mask_zero=True)(txt_input) # shape (max_num_words, 128)\n  # Dropout for less overfitting (still there was quite a bit of overfitting, we did not try other values for dropout%, but would next time)\n  txt_embeding_regularized = tf.keras.layers.Dropout(0.35)(txt_embeding) # shape (max_num_words, 128)\n  # LSTM (with above implemented LSTM_Cell class)\n  txt_128_features = tf.keras.layers.RNN(LSTM_Cell(128))(txt_embeding_regularized) # shape (128)\n\n  \n  # \n  # Decoder\n  #\n\n  # Merge the two encoder models with addition as the merging function.\n  merger = tf.keras.layers.add([img_128_features, txt_128_features]) # shape (128)\n  # Decode features with fully connected layer\n  decoder = tf.keras.layers.Dense(128, activation='relu')(merger) # shape (128)\n  # Output probability distribution over the vocabulary using softmax function.\n  # The highest probability word becomes selected as the next word in the sequence.\n  next_word = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder) # shape (vocab_size)\n\n\n  #\n  # Print and return the full model\n  #\n\n  model = tf.keras.Model(inputs=[img_input_pooled_features, txt_input], outputs=[next_word])\n  print(model.summary())\n  # Keras docs reccomend crossentropy loss and either rmsprop or adam optimizer\n  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n\n  return model"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff1adb79-4006-4169-a8dd-6aa120457a70","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n\n# Adds a tokenizer to caption\ndef tokenize(captions):\n  # Converts dictionary into nested list of captions\n  captionLst = list(captions.values())\n  # Flatmaps the captions so all captions can go into the tokenizer\n  allCaptions = [item for sublist in captionLst for item in sublist]\n  tokenizer = Tokenizer()\n  tokenizer.fit_on_texts(allCaptions)\n  return tokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14d25e75-99f3-4689-9fc8-13a5ed4cc18f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Calculates the longest caption length\ndef calcMaxLength(captions):\n  # Gets the nested list of captions\n  captionLst = list(captions.values())\n  # Flatmaps the captions\n  captionListFlat = [item for sublist in captionLst for item in sublist]\n  maxLength = 0\n  # Iterates and inds the longest caption length\n  for caption in captionListFlat:\n    lengthCheck = len(caption.split())\n    if lengthCheck > maxLength:\n      maxLength = lengthCheck\n  return maxLength\n\n# Removes the longest captions\ndef removeLong(captions):\n  # Gets the nested list of captions\n  captionLst = list(captions.values())\n  keysToRem = []\n  # Iterates through each caption\n  for key, caption in captions.items():\n    for cap in caption:\n      lengthCheck = len(cap.split())\n      # Removes captions that are longer than 20 words\n      if lengthCheck > 20:\n        keysToRem.append(key)\n  for keyRem in keysToRem:\n    captions.pop(keyRem, None)\n  return captions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c506368-14b2-41db-a07f-0878919b0e54","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from numpy import array\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\n# Generates replicated caption data for the model to train on\ndef mapModelData(captions, photoFeats, tokenizer, maxLength, vocabSize):\n  X1 = []\n  X2 = [] \n  y = []\n  # Iterates through each caption (5 per photoId)\n  for photoId, caption in captions.items():\n    partialCaption = []\n    expectedNextWord = []\n    for c in caption:\n      seq = tokenizer.texts_to_sequences([c])[0]\n      # For each caption, the model needs to learn how to predict each word individually.\n      # An m-word caption is replicated into partial captions of length 1, 2,...,m-1 where the \n      # Expected output word is the next word in the original caption.\n      for i in range(1, len(seq)):\n        partialCaption = seq[:i]\n        expectedNextWord = seq[i]\n        # All caption inputs are padded to fill the full input vector.\n        partialCaption = pad_sequences([partialCaption], maxlen=maxLength)[0]\n        # word->integer embedding\n        expectedNextWord = to_categorical([expectedNextWord], num_classes=vocabSize)[0]\n        id = photoId.split(\".\")[0]\n        X1.append(photoFeats[id]) \n        X2.append(partialCaption)\n        y.append(expectedNextWord)\n  return array(X1), array(X2), array(y)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c61d5a8e-e853-4381-b933-31d1cc7de22b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import itertools\nimport pandas as pd\nimport os \n\n# load list of images (16K)\ndirectory = \"/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/flickr30k_images/flickr30k_images/\"\nimagesList = os.listdir(directory)\n\n# loads captions\nloadCaptions = pd.read_csv(\"/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/results.csv\", delimiter='|')\nloadCaptions.columns = ['image_name', 'comment_number', 'comment']\n\n# photo features\n#photoFeats = pd.read_pickle('/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl') \nphotoFeatsPooled = pd.read_pickle('/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/featuresPooled.pkl')\n\n# Formats the captions and removes unecessarily long descriptions\ncaptions = imagesMapCaption(imagesList, loadCaptions)\ndel captions[\"2199200615.jpg\"] #this caption was giving an error. the entry in the dataset had a malformed last caption\nformattedCaptions = formatCaptions(captions)\nremoveLong(formattedCaptions)\n\n# Gets total # of features and # of features we want to train (80%)\ntotalCount = len(formattedCaptions)\ntrainCount = (int) (totalCount * .8)\n\n# Gets 80% of the captions for training\ntrainCaptions = dict(itertools.islice(formattedCaptions.items(), trainCount))\n\n# Gets 20% of the captions for testing\ntestCaptions = dict(itertools.islice(formattedCaptions.items(), trainCount, totalCount))\n\n# Map w/ tokenizer and calculate size\ntokenizer = tokenize(formattedCaptions)\nvocabSize = len(tokenizer.word_index) + 1\n\n# Calculate maximum length in a given segment\nmaxLength = calcMaxLength(formattedCaptions)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7a1b173-88b0-42e7-8e56-c986f9bdffe4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# maps our data for the training\nX1train, X2train, ytrain = mapModelData(trainCaptions, photoFeatsPooled, tokenizer, maxLength, vocabSize)\n\n# maps our data for the testing\nX1test, X2test, ytest = mapModelData(testCaptions, photoFeatsPooled, tokenizer, maxLength, vocabSize)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4cdd9f69-fcb5-4287-904e-8e27f0a23cdd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n\nmodel = mergeModel(vocabSize, maxLength)\n\nfilepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\nmodel.fit([X1train, X2train], ytrain, epochs=10, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7a6b6ad-f6ce-42c1-8aa5-4d946bb93812","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Model: &#34;model&#34;\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 20)]         0           []                               \n                                                                                                  \n input_1 (InputLayer)           [(None, 1280)]       0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 20, 128)      1616512     [&#39;input_2[0][0]&#39;]                \n                                                                                                  \n dropout (Dropout)              (None, 1280)         0           [&#39;input_1[0][0]&#39;]                \n                                                                                                  \n dropout_1 (Dropout)            (None, 20, 128)      0           [&#39;embedding[0][0]&#39;]              \n                                                                                                  \n dense (Dense)                  (None, 128)          163968      [&#39;dropout[0][0]&#39;]                \n                                                                                                  \n rnn (RNN)                      (None, 128)          131584      [&#39;dropout_1[0][0]&#39;]              \n                                                                                                  \n add (Add)                      (None, 128)          0           [&#39;dense[0][0]&#39;,                  \n                                                                  &#39;rnn[0][0]&#39;]                    \n                                                                                                  \n dense_1 (Dense)                (None, 128)          16512       [&#39;add[0][0]&#39;]                    \n                                                                                                  \n dense_2 (Dense)                (None, 12629)        1629141     [&#39;dense_1[0][0]&#39;]                \n                                                                                                  \n==================================================================================================\nTotal params: 3,557,717\nTrainable params: 3,557,717\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n\nEpoch 00001: val_loss improved from inf to 4.64853, saving model to model-ep001-loss4.655-val_loss4.649.h5\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-3059a122-e374-4176-b4fe-68e52c8e27b9/lib/python3.8/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  layer_config = serialize_layer_fn(layer)\n16094/16094 - 567s - loss: 4.6547 - val_loss: 4.6485 - 567s/epoch - 35ms/step\nEpoch 2/10\n\nEpoch 00002: val_loss improved from 4.64853 to 4.52574, saving model to model-ep002-loss4.059-val_loss4.526.h5\n16094/16094 - 567s - loss: 4.0589 - val_loss: 4.5257 - 567s/epoch - 35ms/step\nEpoch 3/10\n\nEpoch 00003: val_loss improved from 4.52574 to 4.52492, saving model to model-ep003-loss3.897-val_loss4.525.h5\n16094/16094 - 567s - loss: 3.8974 - val_loss: 4.5249 - 567s/epoch - 35ms/step\nEpoch 4/10\n\nEpoch 00004: val_loss improved from 4.52492 to 4.50067, saving model to model-ep004-loss3.823-val_loss4.501.h5\n16094/16094 - 568s - loss: 3.8227 - val_loss: 4.5007 - 568s/epoch - 35ms/step\nEpoch 5/10\n\nEpoch 00005: val_loss did not improve from 4.50067\n16094/16094 - 567s - loss: 3.7846 - val_loss: 4.5096 - 567s/epoch - 35ms/step\nEpoch 6/10\n\nEpoch 00006: val_loss did not improve from 4.50067\n16094/16094 - 567s - loss: 3.7581 - val_loss: 4.5157 - 567s/epoch - 35ms/step\nEpoch 7/10\n\nEpoch 00007: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7439 - val_loss: 4.5386 - 569s/epoch - 35ms/step\nEpoch 8/10\n\nEpoch 00008: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7340 - val_loss: 4.5810 - 569s/epoch - 35ms/step\nEpoch 9/10\n\nEpoch 00009: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7278 - val_loss: 4.5713 - 569s/epoch - 35ms/step\nEpoch 10/10\n\nEpoch 00010: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7241 - val_loss: 4.5879 - 569s/epoch - 35ms/step\nOut[9]: &lt;keras.callbacks.History at 0x7f1f078da490&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model: &#34;model&#34;\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 20)]         0           []                               \n                                                                                                  \n input_1 (InputLayer)           [(None, 1280)]       0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 20, 128)      1616512     [&#39;input_2[0][0]&#39;]                \n                                                                                                  \n dropout (Dropout)              (None, 1280)         0           [&#39;input_1[0][0]&#39;]                \n                                                                                                  \n dropout_1 (Dropout)            (None, 20, 128)      0           [&#39;embedding[0][0]&#39;]              \n                                                                                                  \n dense (Dense)                  (None, 128)          163968      [&#39;dropout[0][0]&#39;]                \n                                                                                                  \n rnn (RNN)                      (None, 128)          131584      [&#39;dropout_1[0][0]&#39;]              \n                                                                                                  \n add (Add)                      (None, 128)          0           [&#39;dense[0][0]&#39;,                  \n                                                                  &#39;rnn[0][0]&#39;]                    \n                                                                                                  \n dense_1 (Dense)                (None, 128)          16512       [&#39;add[0][0]&#39;]                    \n                                                                                                  \n dense_2 (Dense)                (None, 12629)        1629141     [&#39;dense_1[0][0]&#39;]                \n                                                                                                  \n==================================================================================================\nTotal params: 3,557,717\nTrainable params: 3,557,717\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n\nEpoch 00001: val_loss improved from inf to 4.64853, saving model to model-ep001-loss4.655-val_loss4.649.h5\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-3059a122-e374-4176-b4fe-68e52c8e27b9/lib/python3.8/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  layer_config = serialize_layer_fn(layer)\n16094/16094 - 567s - loss: 4.6547 - val_loss: 4.6485 - 567s/epoch - 35ms/step\nEpoch 2/10\n\nEpoch 00002: val_loss improved from 4.64853 to 4.52574, saving model to model-ep002-loss4.059-val_loss4.526.h5\n16094/16094 - 567s - loss: 4.0589 - val_loss: 4.5257 - 567s/epoch - 35ms/step\nEpoch 3/10\n\nEpoch 00003: val_loss improved from 4.52574 to 4.52492, saving model to model-ep003-loss3.897-val_loss4.525.h5\n16094/16094 - 567s - loss: 3.8974 - val_loss: 4.5249 - 567s/epoch - 35ms/step\nEpoch 4/10\n\nEpoch 00004: val_loss improved from 4.52492 to 4.50067, saving model to model-ep004-loss3.823-val_loss4.501.h5\n16094/16094 - 568s - loss: 3.8227 - val_loss: 4.5007 - 568s/epoch - 35ms/step\nEpoch 5/10\n\nEpoch 00005: val_loss did not improve from 4.50067\n16094/16094 - 567s - loss: 3.7846 - val_loss: 4.5096 - 567s/epoch - 35ms/step\nEpoch 6/10\n\nEpoch 00006: val_loss did not improve from 4.50067\n16094/16094 - 567s - loss: 3.7581 - val_loss: 4.5157 - 567s/epoch - 35ms/step\nEpoch 7/10\n\nEpoch 00007: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7439 - val_loss: 4.5386 - 569s/epoch - 35ms/step\nEpoch 8/10\n\nEpoch 00008: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7340 - val_loss: 4.5810 - 569s/epoch - 35ms/step\nEpoch 9/10\n\nEpoch 00009: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7278 - val_loss: 4.5713 - 569s/epoch - 35ms/step\nEpoch 10/10\n\nEpoch 00010: val_loss did not improve from 4.50067\n16094/16094 - 569s - loss: 3.7241 - val_loss: 4.5879 - 569s/epoch - 35ms/step\nOut[9]: &lt;keras.callbacks.History at 0x7f1f078da490&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls(\"file:///databricks/driver/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d503c35-36d6-4214-b73d-a362e4e778a5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: [FileInfo(path=&#39;file:/databricks/driver/preload_class.lst&#39;, name=&#39;preload_class.lst&#39;, size=813069),\n FileInfo(path=&#39;file:/databricks/driver/conf/&#39;, name=&#39;conf/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/eventlogs/&#39;, name=&#39;eventlogs/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/model-ep003-loss3.897-val_loss4.525.h5&#39;, name=&#39;model-ep003-loss3.897-val_loss4.525.h5&#39;, size=42769596),\n FileInfo(path=&#39;file:/databricks/driver/model-ep002-loss4.059-val_loss4.526.h5&#39;, name=&#39;model-ep002-loss4.059-val_loss4.526.h5&#39;, size=42769596),\n FileInfo(path=&#39;file:/databricks/driver/logs/&#39;, name=&#39;logs/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/ganglia/&#39;, name=&#39;ganglia/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/model-ep001-loss4.655-val_loss4.649.h5&#39;, name=&#39;model-ep001-loss4.655-val_loss4.649.h5&#39;, size=42769596)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: [FileInfo(path=&#39;file:/databricks/driver/preload_class.lst&#39;, name=&#39;preload_class.lst&#39;, size=813069),\n FileInfo(path=&#39;file:/databricks/driver/conf/&#39;, name=&#39;conf/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/eventlogs/&#39;, name=&#39;eventlogs/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/model-ep003-loss3.897-val_loss4.525.h5&#39;, name=&#39;model-ep003-loss3.897-val_loss4.525.h5&#39;, size=42769596),\n FileInfo(path=&#39;file:/databricks/driver/model-ep002-loss4.059-val_loss4.526.h5&#39;, name=&#39;model-ep002-loss4.059-val_loss4.526.h5&#39;, size=42769596),\n FileInfo(path=&#39;file:/databricks/driver/logs/&#39;, name=&#39;logs/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/ganglia/&#39;, name=&#39;ganglia/&#39;, size=4096),\n FileInfo(path=&#39;file:/databricks/driver/model-ep001-loss4.655-val_loss4.649.h5&#39;, name=&#39;model-ep001-loss4.655-val_loss4.649.h5&#39;, size=42769596)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# dbutils.fs.mv('dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/', '/FileStore/shared_uploads/axc200021@utdallas.edu/model-ep004-loss3.823-val_loss4.501.h5')\ndbutils.fs.ls(\"/FileStore/shared_uploads/axc200021@utdallas.edu/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"168844aa-7b19-4e27-8d47-0edb62c7225e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[23]: [FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl&#39;, name=&#39;features.pkl&#39;, size=4062261449),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/featuresPooled.pkl&#39;, name=&#39;featuresPooled.pkl&#39;, size=83650766),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/flickr30k_images/&#39;, name=&#39;flickr30k_images/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/model-ep004-loss3.823-val_loss4.501.h5&#39;, name=&#39;model-ep004-loss3.823-val_loss4.501.h5&#39;, size=42769596),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/results.csv&#39;, name=&#39;results.csv&#39;, size=13341589)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: [FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/features.pkl&#39;, name=&#39;features.pkl&#39;, size=4062261449),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/featuresPooled.pkl&#39;, name=&#39;featuresPooled.pkl&#39;, size=83650766),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/flickr30k_images/&#39;, name=&#39;flickr30k_images/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/model-ep004-loss3.823-val_loss4.501.h5&#39;, name=&#39;model-ep004-loss3.823-val_loss4.501.h5&#39;, size=42769596),\n FileInfo(path=&#39;dbfs:/FileStore/shared_uploads/axc200021@utdallas.edu/results.csv&#39;, name=&#39;results.csv&#39;, size=13341589)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from numpy import argmax\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Generate a description for an image\ndef imgCaption(model, photoFeatsPooled, maxLength, tokenizer):\n  # Gets tokenizer's dictionary of word indexes\n  tokenizerDict = tokenizer.word_index\n  # Starts with beginning token\n  textSeq = '<begofseq>'\n  for i in range(maxLength):\n    sequence = tokenizer.texts_to_sequences([textSeq])[0]\n\n    sequence = pad_sequences([sequence], maxlen=maxLength)[0]\n\n    input1 = tf.convert_to_tensor([photoFeatsPooled], dtype=tf.float32)\n    input2 = tf.convert_to_tensor([sequence], dtype=tf.int64)\n    \n    # predict next word\n    genInt = model.predict([input1, input2])\n    \n    # Gets word with highest probability\n    genInt = argmax(genInt)\n    \n    if genInt in tokenizerDict:\n        genWord = tokenizerDict[genInt]\n    else:\n        break\n    # Ends caption if the end of it is reached\n    if genWord == 'endofseq':\n      textSeq += ' <' + genWord + '>'\n      break\n    textSeq += ' ' + genWord\n  return textSeq\n\ndef modelEvaluation(model, captions, photoFeatsPooled, maxLength, tokenizer):\n  original, predicted = [], []\n  for key, caption in captions.items():\n    # model generates description\n    genCaption = imgCaption(model, photoFeatsPooled[key[:-4]], maxLength, tokenizer)\n    references = [c.split() for c in caption]\n    original.append(references)\n    predicted.append(genCaption.split())\n  print('BLEU-1: %f' % corpus_bleu(original, predicted, weights=(1.0, 0, 0, 0)))\n  print('BLEU-2: %f' % corpus_bleu(original, predicted, weights=(0.5, 0.5, 0, 0)))\n  print('BLEU-3: %f' % corpus_bleu(original, predicted, weights=(0.3, 0.3, 0.3, 0)))\n  print('BLEU-4: %f' % corpus_bleu(original, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d48fcead-aa14-47a1-9714-6645a3ae2f66","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# load new instance of the model for testing and evaluation\nmodelTest = mergeModel(vocabSize, maxLength)\nfilename = '/dbfs/FileStore/shared_uploads/axc200021@utdallas.edu/model-ep004-loss3.823-val_loss4.501.h5'\nmodelTest.load_weights(filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d171d70-5dcc-46ae-8909-beff43d59cc5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Model: &#34;model&#34;\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 20)]         0           []                               \n                                                                                                  \n input_1 (InputLayer)           [(None, 1280)]       0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 20, 128)      1616512     [&#39;input_2[0][0]&#39;]                \n                                                                                                  \n dropout (Dropout)              (None, 1280)         0           [&#39;input_1[0][0]&#39;]                \n                                                                                                  \n dropout_1 (Dropout)            (None, 20, 128)      0           [&#39;embedding[0][0]&#39;]              \n                                                                                                  \n dense (Dense)                  (None, 128)          163968      [&#39;dropout[0][0]&#39;]                \n                                                                                                  \n rnn (RNN)                      (None, 128)          131584      [&#39;dropout_1[0][0]&#39;]              \n                                                                                                  \n add (Add)                      (None, 128)          0           [&#39;dense[0][0]&#39;,                  \n                                                                  &#39;rnn[0][0]&#39;]                    \n                                                                                                  \n dense_1 (Dense)                (None, 128)          16512       [&#39;add[0][0]&#39;]                    \n                                                                                                  \n dense_2 (Dense)                (None, 12629)        1629141     [&#39;dense_1[0][0]&#39;]                \n                                                                                                  \n==================================================================================================\nTotal params: 3,557,717\nTrainable params: 3,557,717\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model: &#34;model&#34;\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 20)]         0           []                               \n                                                                                                  \n input_1 (InputLayer)           [(None, 1280)]       0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 20, 128)      1616512     [&#39;input_2[0][0]&#39;]                \n                                                                                                  \n dropout (Dropout)              (None, 1280)         0           [&#39;input_1[0][0]&#39;]                \n                                                                                                  \n dropout_1 (Dropout)            (None, 20, 128)      0           [&#39;embedding[0][0]&#39;]              \n                                                                                                  \n dense (Dense)                  (None, 128)          163968      [&#39;dropout[0][0]&#39;]                \n                                                                                                  \n rnn (RNN)                      (None, 128)          131584      [&#39;dropout_1[0][0]&#39;]              \n                                                                                                  \n add (Add)                      (None, 128)          0           [&#39;dense[0][0]&#39;,                  \n                                                                  &#39;rnn[0][0]&#39;]                    \n                                                                                                  \n dense_1 (Dense)                (None, 128)          16512       [&#39;add[0][0]&#39;]                    \n                                                                                                  \n dense_2 (Dense)                (None, 12629)        1629141     [&#39;dense_1[0][0]&#39;]                \n                                                                                                  \n==================================================================================================\nTotal params: 3,557,717\nTrainable params: 3,557,717\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["modelEvaluation(modelTest, testCaptions, photoFeatsPooled, tokenizer, maxLength)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b89b6853-15df-44ef-ae49-31782a180b7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">BLEU-1: 0.547964\nBLEU-2: 0.298427\nBLEU-3: 0.209437\nBLEU-4: 0.098722\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">BLEU-1: 0.547964\nBLEU-2: 0.298427\nBLEU-3: 0.209437\nBLEU-4: 0.098722\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["\ncaptionsToS = dict(list(testCaptions.items())[:10])\n\nfor photoId, captions in captionsToS.items():\n  yhat = imgCaption(modelTest, photoFeatsPooled[photoId[:-4]], maxLength, tokenizer)\n  print(\"Key: \", photoId)\n  print(\"Original: \", captions)\n  print(\"Prediction: \", yhat)\n  print('\\n')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b85672d-38b8-4877-9f5e-5df8e42a6783","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Key:  4860699848.jpg\nOriginal:  [&#39;&lt;begofseq&gt; little boy plays in water that has formed puddle &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; little kid in black playing in fountain &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; young child is playing in fountain &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; little boy is walking through water &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; boy is playing in water fountain &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4860765462.jpg\nOriginal:  [&#39;&lt;begofseq&gt; young woman wearing floral dress and another young woman wearing black shirt and jeans walk together &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in dress and cowboy boots is walking next to another woman in black tshirt &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two young asian women walk in stride on crowded street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two women with sunhats walk down city street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two women walking down the street &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4860805264.jpg\nOriginal:  [&#39;&lt;begofseq&gt; an asian man wearing white tshirt and an asian woman wearing skirt are walking down busy sidewalk &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in long floral skirt carrying basket walks in crowd of people down city street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; asian woman with purse stares off into distance in crowded street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; plain woman in dress standing in the street facing the sun &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in long skirt and man walk down the street &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  48614561.jpg\nOriginal:  [&#39;&lt;begofseq&gt; two babies are sitting on the floor next to the table and one baby is crying &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; one toddler takes away the mouth piece of the other which makes him cry &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; the baby in the yellow shirt cries while reaching out to another baby &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two asian toddlers one crying playing with pacifier &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; baby cries while another baby holds an object &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4861590390.jpg\nOriginal:  [&#39;&lt;begofseq&gt; lady in bluegreen costume standing outside &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in blue green and gold costume &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; costume with vibrant blue and green colors &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; lady wearing celebratory bug dress &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman is wearing peacock costume &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4861870951.jpg\nOriginal:  [&#39;&lt;begofseq&gt; man in blue jacket and khaki pants bicycling on sidewalk next to tree &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man wearing blue jacket is riding bicycle in an empty park &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man in blue jacket rides bicycle on the sidewalk &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man in blue coat rides bicycle along sidewalk &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man rides bike past an old baseball field &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4861894586.jpg\nOriginal:  [&#39;&lt;begofseq&gt; asian kids who are wearing backpacks are standing in front of yellow school bus &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; group of students are outside waiting in front of yellow school bus &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; group of students wearing backpacks wait at bus stop &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; bunch of kids waiting for the school bus &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; students sit outside by school bus &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4862204000.jpg\nOriginal:  [&#39;&lt;begofseq&gt; an older man sites on cushioned bench with his hands crossed in his lap &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man rests his eyes sitting on red couch with his hands folded &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; old man wearing hat and coat sleeping sitting up on sofa &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; an elderly man sleeps sitting up on the end of red couch &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; an old man is sitting alone on couch and sleeping &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4862440150.jpg\nOriginal:  [&#39;&lt;begofseq&gt; one person is exiting the bottom of long escalator while another has just entered at the bottom &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man walking away from an escalator while woman is coming up the escalator &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; person in black has just stepped off of the escalator &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; person walking away from the bottom of an escalator &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; silhouette at the bottom of an escalator &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4862788297.jpg\nOriginal:  [&#39;&lt;begofseq&gt; an asian child looking towards something wearing traditional clothing &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; young female child carrying light blue bag around her shoulders &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; little oriental girl glancing upward toward her right &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; the little girl looked in awe at the sight &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; native girl in blue top looking skyward &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Key:  4860699848.jpg\nOriginal:  [&#39;&lt;begofseq&gt; little boy plays in water that has formed puddle &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; little kid in black playing in fountain &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; young child is playing in fountain &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; little boy is walking through water &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; boy is playing in water fountain &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4860765462.jpg\nOriginal:  [&#39;&lt;begofseq&gt; young woman wearing floral dress and another young woman wearing black shirt and jeans walk together &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in dress and cowboy boots is walking next to another woman in black tshirt &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two young asian women walk in stride on crowded street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two women with sunhats walk down city street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two women walking down the street &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4860805264.jpg\nOriginal:  [&#39;&lt;begofseq&gt; an asian man wearing white tshirt and an asian woman wearing skirt are walking down busy sidewalk &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in long floral skirt carrying basket walks in crowd of people down city street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; asian woman with purse stares off into distance in crowded street &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; plain woman in dress standing in the street facing the sun &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in long skirt and man walk down the street &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  48614561.jpg\nOriginal:  [&#39;&lt;begofseq&gt; two babies are sitting on the floor next to the table and one baby is crying &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; one toddler takes away the mouth piece of the other which makes him cry &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; the baby in the yellow shirt cries while reaching out to another baby &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; two asian toddlers one crying playing with pacifier &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; baby cries while another baby holds an object &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4861590390.jpg\nOriginal:  [&#39;&lt;begofseq&gt; lady in bluegreen costume standing outside &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman in blue green and gold costume &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; costume with vibrant blue and green colors &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; lady wearing celebratory bug dress &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; woman is wearing peacock costume &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4861870951.jpg\nOriginal:  [&#39;&lt;begofseq&gt; man in blue jacket and khaki pants bicycling on sidewalk next to tree &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man wearing blue jacket is riding bicycle in an empty park &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man in blue jacket rides bicycle on the sidewalk &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man in blue coat rides bicycle along sidewalk &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man rides bike past an old baseball field &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4861894586.jpg\nOriginal:  [&#39;&lt;begofseq&gt; asian kids who are wearing backpacks are standing in front of yellow school bus &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; group of students are outside waiting in front of yellow school bus &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; group of students wearing backpacks wait at bus stop &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; bunch of kids waiting for the school bus &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; students sit outside by school bus &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4862204000.jpg\nOriginal:  [&#39;&lt;begofseq&gt; an older man sites on cushioned bench with his hands crossed in his lap &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man rests his eyes sitting on red couch with his hands folded &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; old man wearing hat and coat sleeping sitting up on sofa &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; an elderly man sleeps sitting up on the end of red couch &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; an old man is sitting alone on couch and sleeping &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4862440150.jpg\nOriginal:  [&#39;&lt;begofseq&gt; one person is exiting the bottom of long escalator while another has just entered at the bottom &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; man walking away from an escalator while woman is coming up the escalator &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; person in black has just stepped off of the escalator &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; person walking away from the bottom of an escalator &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; silhouette at the bottom of an escalator &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\nKey:  4862788297.jpg\nOriginal:  [&#39;&lt;begofseq&gt; an asian child looking towards something wearing traditional clothing &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; young female child carrying light blue bag around her shoulders &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; little oriental girl glancing upward toward her right &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; the little girl looked in awe at the sight &lt;endofseq&gt;&#39;, &#39;&lt;begofseq&gt; native girl in blue top looking skyward &lt;endofseq&gt;&#39;]\nPrediction:  &lt;begofseq&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c1b7f7d-5e62-4d95-b281-311face5c22d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CS 6350 Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":902883914968575}},"nbformat":4,"nbformat_minor":0}
